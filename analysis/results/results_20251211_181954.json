{
  "metadata": {
    "embedding_type": "bert",
    "embedding_dim": 768,
    "num_classes": 6,
    "timestamp": "20251211_181954",
    "cross_validation": {
      "enabled": true,
      "n_splits": 5,
      "shuffle": true,
      "random_state": 42
    }
  },
  "results": {
    "MLP": {
      "test_accuracy": 74.8151798915722,
      "history": {
        "train_loss": [
          1.4308132444109236,
          1.011950054339,
          0.8552361045564923,
          0.767456271818706,
          0.7083557588713509,
          0.6581235485417503,
          0.6126923859119415,
          0.5787793397903442,
          0.5742128150803703,
          0.5710985064506531,
          0.5665935107639858
        ],
        "train_accuracy": [
          46.88028169014085,
          66.5,
          71.55633802816901,
          73.9225352112676,
          75.66901408450704,
          77.83098591549296,
          79.28873239436619,
          80.65492957746478,
          80.7394366197183,
          80.77464788732394,
          80.95774647887323
        ],
        "val_loss": [
          1.605838418006897,
          1.1221455931663513,
          0.8651248812675476,
          0.7895570397377014,
          0.7554164826869965,
          0.7380956411361694,
          0.725838840007782,
          0.7228138148784637,
          0.7223276495933533,
          0.7212653458118439,
          0.721417248249054
        ],
        "val_accuracy": [
          60.177427304090685,
          68.55593888615081,
          71.21734844751109,
          72.59733859043864,
          73.33661902414983,
          74.51946771808772,
          74.12518482010843,
          74.4208969935929,
          74.32232626909808,
          74.37161163134549,
          74.12518482010843
        ]
      },
      "classification_report_text": "              precision    recall  f1-score   support\n\n       CRIME      0.811     0.850     0.830       712\n     DIVORCE      0.777     0.788     0.783       685\n      IMPACT      0.651     0.671     0.661       697\n       MEDIA      0.750     0.749     0.749       589\n       WOMEN      0.676     0.628     0.651       715\n  WORLD NEWS      0.825     0.808     0.816       660\n\n    accuracy                          0.748      4058\n   macro avg      0.748     0.749     0.748      4058\nweighted avg      0.747     0.748     0.748      4058\n",
      "classification_report": {
        "CRIME": {
          "precision": 0.8109919571045576,
          "recall": 0.8497191011235955,
          "f1-score": 0.8299039780521262,
          "support": 712.0
        },
        "DIVORCE": {
          "precision": 0.7769784172661871,
          "recall": 0.7883211678832117,
          "f1-score": 0.782608695652174,
          "support": 685.0
        },
        "IMPACT": {
          "precision": 0.650904033379694,
          "recall": 0.6714490674318508,
          "f1-score": 0.6610169491525424,
          "support": 697.0
        },
        "MEDIA": {
          "precision": 0.75,
          "recall": 0.7487266553480475,
          "f1-score": 0.7493627867459643,
          "support": 589.0
        },
        "WOMEN": {
          "precision": 0.6762048192771084,
          "recall": 0.627972027972028,
          "f1-score": 0.6511965192168238,
          "support": 715.0
        },
        "WORLD NEWS": {
          "precision": 0.8250773993808049,
          "recall": 0.8075757575757576,
          "f1-score": 0.8162327718223583,
          "support": 660.0
        },
        "accuracy": 0.748151798915722,
        "macro avg": {
          "precision": 0.7483594377347255,
          "recall": 0.7489606295557486,
          "f1-score": 0.7483869501069981,
          "support": 4058.0
        },
        "weighted avg": {
          "precision": 0.7474431074238279,
          "recall": 0.748151798915722,
          "f1-score": 0.747511390953754,
          "support": 4058.0
        }
      },
      "cv": {
        "folds": [
          {
            "fold": 1,
            "best_val_acc": 75.38196155741744,
            "best_val_loss": 0.7051070630550385
          },
          {
            "fold": 2,
            "best_val_acc": 75.85017249876786,
            "best_val_loss": 0.6926681399345398
          },
          {
            "fold": 3,
            "best_val_acc": 75.91816613261031,
            "best_val_loss": 0.6643827557563782
          },
          {
            "fold": 4,
            "best_val_acc": 75.42519102785309,
            "best_val_loss": 0.681425005197525
          },
          {
            "fold": 5,
            "best_val_acc": 74.58713334976584,
            "best_val_loss": 0.698981449007988
          }
        ],
        "mean_val_acc": 75.43252491328292,
        "std_val_acc": 0.4748840351220354
      }
    }
  }
}